{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# import\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import autograd_extended\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Input, Lambda, Dense, RepeatVector\n",
    "from keras import backend as K\n",
    "from keras import objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the latent ODE model\n",
    "<strong>From appendix D</strong>\n",
    "\n",
    "To obtain the latent representation $z_{t_{0}}$, we traverse using RNN and obtain parameters of distribution $ q(\\textbf{z}_{t_{0}} | \\{ \\textbf{x}_{t_{i}}, t_{i} \\}_{i}, \\theta_{enc})$. The algorithm is the following:\n",
    "\n",
    "\n",
    "## Step 1\n",
    "Run an RNN encoder through the time series and infer the parameters for the a posterior over $ \\textbf{z}_{t_{0}}$:\n",
    "$$ q(\\textbf{z}_{t_{0}} | \\{ \\textbf{x}_{t_{i}}, t_{i} \\}_{i}, \\phi) = \\mathcal{N}(\\textbf{z}_{t_{0}} | \\mu_{\\textbf{z}_{t_{0}}}, \\sigma_{\\textbf{z}_{0}}) $$\n",
    "\n",
    "where $\\mu_{z_{0}}, \\sigma_{z_{0}}$ comes from hidden state of $ RNN(\\{ \\textbf{x}_{t_{i}} , t_{i} \\}_{i}, \\phi) $\n",
    "\n",
    "## Step 2\n",
    "Sample $ \\textbf{z}_{t_{0}} \\sim q(\\textbf{z}_{t_{0}} | \\{ \\textbf{x}_{t_{i}}, t_{i} \\}_{i}) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process Data\n",
    "\n",
    "# obtain simple time series with sine function\n",
    "df = pd.read_csv('./data.csv')\n",
    "\n",
    "x, y = df[\"x\"], df[\"y\"]\n",
    "\n",
    "# split y into timesteps for training (-1, 100, 1) we use a sliding window\n",
    "timesteps = 5\n",
    "\n",
    "newY = []\n",
    "for i in range(0, len(y)-timesteps):\n",
    "    temp = y[i:i+timesteps]\n",
    "    \n",
    "    temp_ls = []\n",
    "    for j in temp:\n",
    "        temp_ls.append([j])\n",
    "    newY.append(temp_ls)\n",
    "    \n",
    "y = np.array(newY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and run the autoencoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, 5, 1)              0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 32)                4352      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               3300      \n",
      "=================================================================\n",
      "Total params: 7,652\n",
      "Trainable params: 7,652\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "repeat_vector_2 (RepeatVecto (None, 5, 100)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 5, 32)             17024     \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 5, 1)              136       \n",
      "=================================================================\n",
      "Total params: 17,160\n",
      "Trainable params: 17,160\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, 5, 1)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 32)           4352        encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          3300        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 100)          3300        lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 100)          0           dense_1[0][0]                    \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_1 (RepeatVector)  (None, 5, 100)       0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                   (None, 5, 32)        17024       repeat_vector_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   (None, 5, 1)         136         lstm_2[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 28,112\n",
      "Trainable params: 28,112\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Run an RNN encoder on the time series\n",
    "\n",
    "# Params\n",
    "timesteps = 5\n",
    "input_dim = 1\n",
    "lstm_dim = 32\n",
    "latent_dim = 100\n",
    "batch_size = 1\n",
    "\n",
    "# Encoder model\n",
    "def sample_z(args):\n",
    "    \"\"\"Reparameterization trick by sampling fr an isotropic unit Gaussian.\n",
    "    # Arguments:\n",
    "        args (tensor): mean and log of variance of Q(z|X)\n",
    "    # Returns:\n",
    "        z (tensor): sampled latent vector\n",
    "    \"\"\"\n",
    "\n",
    "    z_mean, z_log_var = args\n",
    "    batch = K.shape(z_mean)[0]\n",
    "    dim = K.int_shape(z_mean)[1]\n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = K.random_normal(shape=(batch, dim))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "\n",
    "# Input\n",
    "inputs = Input(shape=(timesteps, input_dim,), name='encoder_input')\n",
    "\n",
    "# LSTM encoding\n",
    "h = LSTM(lstm_dim)(inputs)\n",
    "\n",
    "# VAE Z layer\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_sigma = Dense(latent_dim)(h)\n",
    "\n",
    "# Obtain z\n",
    "z = Lambda(sample_z, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "\n",
    "encoder = Model(inputs, z_mean, name=\"encoder\")\n",
    "\n",
    "# Decoder model\n",
    "decoder_h = LSTM(lstm_dim, return_sequences=True)\n",
    "decoder_mean = LSTM(input_dim, return_sequences=True)\n",
    "\n",
    "h_decoded = RepeatVector(timesteps)(z)\n",
    "h_decoded = decoder_h(h_decoded)\n",
    "\n",
    "# decoded layer\n",
    "outputs = decoder_mean(h_decoded)\n",
    "\n",
    "# decoder, from latent space to reconstructed inputs\n",
    "decoder_inputs = Input(shape=(latent_dim,))\n",
    "\n",
    "_h_decoded = RepeatVector(timesteps)(decoder_inputs)\n",
    "_h_decoded = decoder_h(_h_decoded)\n",
    "\n",
    "decoder_outputs = decoder_mean(_h_decoded)\n",
    "\n",
    "decoder = Model(decoder_inputs, decoder_outputs)\n",
    "\n",
    "# Build vae\n",
    "vae = Model(inputs, outputs, name='vae_lstm')\n",
    "def vae_loss(x, x_decoded_mean):\n",
    "    xent_loss = objectives.mse(x, x_decoded_mean)\n",
    "    kl_loss = - 0.5 * K.mean(1 + z_log_sigma - K.square(z_mean) - K.exp(z_log_sigma))\n",
    "    loss = xent_loss + kl_loss\n",
    "    return loss\n",
    "\n",
    "vae.compile(optimizer='adam', loss=vae_loss)\n",
    "\n",
    "encoder.summary()\n",
    "decoder.summary()\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1995/1995 [==============================] - 6s 3ms/step - loss: 0.5021\n",
      "Epoch 2/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.4749\n",
      "Epoch 3/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.4643\n",
      "Epoch 4/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.4568\n",
      "Epoch 5/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.4422\n",
      "Epoch 6/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.3795\n",
      "Epoch 7/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.2682\n",
      "Epoch 8/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.2233\n",
      "Epoch 9/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.2022\n",
      "Epoch 10/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1923\n",
      "Epoch 11/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1809\n",
      "Epoch 12/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1736\n",
      "Epoch 13/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1697\n",
      "Epoch 14/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1614\n",
      "Epoch 15/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1550\n",
      "Epoch 16/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1526\n",
      "Epoch 17/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1449\n",
      "Epoch 18/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1434\n",
      "Epoch 19/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1373\n",
      "Epoch 20/20\n",
      "1995/1995 [==============================] - 2s 1ms/step - loss: 0.1364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa94d0b75f8>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the VAE to obtain z\n",
    "vae.fit(y, y, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: (1995, 5, 1), preds: (1995, 5, 1)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 4 is out of bounds for axis 2 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d61cb65debd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# pick a column to plot.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"x: %s, preds: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 4 is out of bounds for axis 2 with size 1"
     ]
    }
   ],
   "source": [
    "# Sanity check, we should get a sine graph\n",
    "prediction = vae.predict(y, batch_size=batch_size)\n",
    "\n",
    "# pick a column to plot.\n",
    "print(\"x: %s, preds: %s\" % (y.shape, prediction.shape))\n",
    "plt.plot(y[:,1], label='data')\n",
    "plt.plot(prediction[:,0,1], label='predict')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "Obtain $\\textbf{z}_{t_{1}}, \\textbf{z}_{t_{2}}, ..., \\textbf{z}_{t_{M}} $ by solving ODE $ ODESolve(\\textbf{z}_{t_{0}}, f, \\theta_{f}, t_{0},...,t_{M})$ where $f$ is the function defining the gradient $d\\textbf{z}/{dt}$ as a function of $\\textbf{z}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "Maximize ELBO (Evidence Lower Bound) \n",
    "$$ ELBO = \\sum^{M}_{i=1}\n",
    "        {\n",
    "             \\log p(\\textbf{x}_{t_{i}}) | \\textbf{z}_{t_{i}}, \\theta_{\\textbf{x}} ) \n",
    "             + \\log p(\\textbf{z}_{t_{0}})\n",
    "             - \\log q(\\textbf{z}_{t_{0}} | \\{ \\textbf{x}_{t_{i}}, t_{i} \\}_{i}, \\phi)\n",
    "        }  \n",
    "$$\n",
    "\n",
    "Where $p(\\textbf{z}_{t_{0}}) = \\mathcal{N}(0,1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:catalyst]",
   "language": "python",
   "name": "conda-env-catalyst-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
